{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "source_text = \"αυτή είναι μια πάρα πολύ δύσκολη μέρα\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "# tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# encoded_zh = tokenizer(source_text, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = (time.time() * 1000)\n",
    "# generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "# dt = (time.time() * 1000)-t1\n",
    "# # print(pred)\n",
    "# print(dt,\"ms\")\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a very difficult day.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, pipeline, AutoProcessor\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "translator = pipeline(\"translation_el_to_en\",model=model, tokenizer=tokenizer, device=0)\n",
    "translator(source_text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very difficult day.\n",
      "89.31591796875 ms\n"
     ]
    }
   ],
   "source": [
    "t1 = (time.time() * 1000)\n",
    "pred = translator(source_text)[0]['translation_text']\n",
    "dt = (time.time() * 1000)-t1\n",
    "print(pred)\n",
    "print(dt,\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TensorrtExecutionProvider',\n",
       " 'CUDAExecutionProvider',\n",
       " 'AzureExecutionProvider',\n",
       " 'CPUExecutionProvider']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort.get_device()\n",
    "ort.get_available_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/giorgospap/onnx/test.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_onnx_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/giorgospap/onnx/onnx2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_base_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfacebook/m2m100_418M\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model_onnx \u001b[39m=\u001b[39m ORTModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/m2m100_418M\u001b[39;49m\u001b[39m\"\u001b[39;49m,export\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizers \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/m2m100_418M\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# model_onnx.save_pretrained(model_onnx_path)\u001b[39;00m\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py:622\u001b[0m, in \u001b[0;36mORTModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    574\u001b[0m \u001b[39m@add_start_docstrings\u001b[39m(FROM_PRETRAINED_START_DOCSTRING)\n\u001b[1;32m    575\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    590\u001b[0m ):\n\u001b[1;32m    591\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39m    provider (`str`, defaults to `\"CPUExecutionProvider\"`):\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m        ONNX Runtime provider to use for loading the model. See https://onnxruntime.ai/docs/execution-providers/ for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39m        `ORTModel`: The loaded ORTModel model.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    623\u001b[0m         model_id,\n\u001b[1;32m    624\u001b[0m         export\u001b[39m=\u001b[39;49mexport,\n\u001b[1;32m    625\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    626\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    627\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    628\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    629\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    630\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    631\u001b[0m         provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m    632\u001b[0m         session_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    633\u001b[0m         provider_options\u001b[39m=\u001b[39;49mprovider_options,\n\u001b[1;32m    634\u001b[0m         use_io_binding\u001b[39m=\u001b[39;49muse_io_binding,\n\u001b[1;32m    635\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    636\u001b[0m     )\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/optimum/modeling_base.py:372\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     trust_remote_code \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    371\u001b[0m from_pretrained_method \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_transformers \u001b[39mif\u001b[39;00m export \u001b[39melse\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained\n\u001b[0;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m from_pretrained_method(\n\u001b[1;32m    373\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    374\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    375\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    376\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    377\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    378\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    379\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    380\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    381\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    382\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/optimum/onnxruntime/modeling_seq2seq.py:1052\u001b[0m, in \u001b[0;36mORTModelForConditionalGeneration._from_transformers\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, subfolder, local_files_only, trust_remote_code, use_cache, use_merged, provider, session_options, provider_options, use_io_binding, task)\u001b[0m\n\u001b[1;32m   1049\u001b[0m save_dir \u001b[39m=\u001b[39m TemporaryDirectory()\n\u001b[1;32m   1050\u001b[0m save_dir_path \u001b[39m=\u001b[39m Path(save_dir\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1052\u001b[0m main_export(\n\u001b[1;32m   1053\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m   1054\u001b[0m     output\u001b[39m=\u001b[39;49msave_dir_path,\n\u001b[1;32m   1055\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m   1056\u001b[0m     do_validation\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1057\u001b[0m     no_post_process\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m use_merged,\n\u001b[1;32m   1058\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1059\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1060\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1061\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1062\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1063\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1064\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m   1065\u001b[0m )\n\u001b[1;32m   1067\u001b[0m config\u001b[39m.\u001b[39msave_pretrained(save_dir_path)\n\u001b[1;32m   1068\u001b[0m maybe_save_preprocessors(model_id, save_dir_path, src_subfolder\u001b[39m=\u001b[39msubfolder)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/optimum/exporters/onnx/__main__.py:356\u001b[0m, in \u001b[0;36mmain_export\u001b[0;34m(model_name_or_path, output, task, opset, device, fp16, optimize, monolith, no_post_process, framework, atol, cache_dir, trust_remote_code, pad_token_id, subfolder, revision, force_download, local_files_only, use_auth_token, for_ort, do_validation, model_kwargs, custom_onnx_configs, fn_get_submodels, use_subprocess, _variant, library_name, legacy, **kwargs_shapes)\u001b[0m\n\u001b[1;32m    351\u001b[0m         model_tasks \u001b[39m=\u001b[39m TasksManager\u001b[39m.\u001b[39mget_supported_tasks_for_model_type(model_type, exporter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    352\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    353\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAsked to export a \u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m model for the task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mautodetected_message\u001b[39m}\u001b[39;00m\u001b[39m, but the Optimum ONNX exporter only supports the tasks \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(model_tasks\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. Please use a supported task. Please open an issue at https://github.com/huggingface/optimum/issues if you would like the task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m to be supported in the ONNX export for \u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0m model \u001b[39m=\u001b[39m TasksManager\u001b[39m.\u001b[39;49mget_model_from_task(\n\u001b[1;32m    357\u001b[0m     task,\n\u001b[1;32m    358\u001b[0m     model_name_or_path,\n\u001b[1;32m    359\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    360\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    361\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    362\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    363\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    364\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    365\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    366\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    367\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m    368\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    369\u001b[0m     library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m is_stable_diffusion \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstable-diffusion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m task\n\u001b[1;32m    373\u001b[0m model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstable-diffusion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_stable_diffusion \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel_type\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/optimum/exporters/tasks.py:1750\u001b[0m, in \u001b[0;36mTasksManager.get_model_from_task\u001b[0;34m(task, model_name_or_path, subfolder, revision, framework, cache_dir, torch_dtype, device, library_name, **model_kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m library_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiffusers\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1747\u001b[0m     \u001b[39mwith\u001b[39;00m device:\n\u001b[1;32m   1748\u001b[0m         \u001b[39m# Initialize directly in the requested device, to save allocation time. Especially useful for large\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         \u001b[39m# models to initialize on cuda device.\u001b[39;00m\n\u001b[0;32m-> 1750\u001b[0m         model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1751\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/transformers/modeling_utils.py:3236\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[39m=\u001b[39mtorch_dtype, device_map\u001b[39m=\u001b[39mdevice_map)\n\u001b[1;32m   3235\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3236\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   3238\u001b[0m \u001b[39m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:1220\u001b[0m, in \u001b[0;36mM2M100ForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: M2M100Config):\n\u001b[1;32m   1219\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m-> 1220\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m M2M100Model(config)\n\u001b[1;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mnum_embeddings, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1223\u001b[0m     \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:1104\u001b[0m, in \u001b[0;36mM2M100Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m   1103\u001b[0m padding_idx, vocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpad_token_id, config\u001b[39m.\u001b[39mvocab_size\n\u001b[0;32m-> 1104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(vocab_size, config\u001b[39m.\u001b[39;49md_model, padding_idx)\n\u001b[1;32m   1106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m M2M100Encoder(config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared)\n\u001b[1;32m   1107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m M2M100Decoder(config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/nn/modules/sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(_weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[1;32m    147\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/nn/modules/sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     init\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/nn/init.py:154\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Fills the input Tensor with values drawn from the normal\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mdistribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39m    >>> nn.init.normal_(w)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49moverrides\u001b[39m.\u001b[39;49mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39;49mtensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd)\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1560\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1561\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "# from optimum.pipelines import pipeline\n",
    "model_onnx_path = \"/home/giorgospap/onnx/onnx2\"\n",
    "model_base_id = \"facebook/m2m100_418M\"\n",
    "model_onnx = ORTModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\",export=True)\n",
    "tokenizers = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "# model_onnx.save_pretrained(model_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 13:04:53.552924714 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-11-22 13:04:53.552939127 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n",
      "2023-11-22 13:04:55.815517639 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-11-22 13:04:55.815543306 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n",
      "2023-11-22 13:04:57.405531936 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-11-22 13:04:57.405543526 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "translator = pipeline(\"translation_el_to_en\",model=model_onnx,tokenizer=tokenizers, device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very difficult day.\n",
      "152.038330078125 ms\n"
     ]
    }
   ],
   "source": [
    "t1 = (time.time() * 1000)\n",
    "pred = translator(source_text)[0]['translation_text']\n",
    "dt = (time.time() * 1000)-t1\n",
    "print(pred)\n",
    "print(dt,\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:141: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_pos > self.weights.size(0):\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:266: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:273: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:305: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Using framework PyTorch: 2.1.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:66: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "Using framework PyTorch: 2.1.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:228: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:770: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Optimizing model...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import (\n",
    "    AutoOptimizationConfig, ORTOptimizer, ORTModelForSeq2SeqLM\n",
    ")\n",
    "save_dir = \"distilbert_optimized\"\n",
    "model_base_id = \"facebook/m2m100_418M\"\n",
    "\n",
    "# Load a PyTorch model and export it to the ONNX format\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(model_base_id, export=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "\n",
    "# Define the optimization strategy by creating the appropriate configuration\n",
    "optimization_config = AutoOptimizationConfig.O2()\n",
    "\n",
    "# Optimize the model\n",
    "optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import OptimizationConfig, ORTOptimizer, ORTModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"sshleifer/distilbart-cnn-12-6\"\n",
    "save_dir = \"distilbart_optimized\"\n",
    "\n",
    "# Load a PyTorch model and export it to the ONNX format\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(model_id, export=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "\n",
    "# Define the optimization strategy by creating the appropriate configuration\n",
    "optimization_config = OptimizationConfig(\n",
    "    optimization_level=2,\n",
    "    enable_transformers_specific_optimizations=True,\n",
    "    optimize_for_gpu=False,\n",
    ")\n",
    "\n",
    "# Optimize the model\n",
    "optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "optimized_model = ORTModelForSeq2SeqLM.from_pretrained(save_dir)\n",
    "tokens = tokenizer(\"This is a sample input\", return_tensors=\"pt\")\n",
    "outputs = optimized_model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:53:00.100379069 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-11-20 14:53:00.100399280 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.optimizer import optimize_model\n",
    "from pathlib import Path\n",
    "# Create a PATH to save the model\n",
    "model_onnx_path = \"/home/giorgospap/onnx/onnx2\"\n",
    "model_onnx_path = Path(model_onnx_path)\n",
    "\n",
    "# Create a PATH to save the optimized model\n",
    "optimized_onnx_path = model_onnx_path / \"decoder_with_past_optimized.onnx\"\n",
    "\n",
    "# Optimize the model using ORT transformer optimizer with its default values\n",
    "optimized_model = optimize_model(\n",
    "    input=str(model_onnx_path / \"decoder_with_past_model.onnx\"),\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "optimized_model.save_model_to_file(optimized_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optimum.exporters.onnx import main_export\n",
    "# from optimum.exporters.onnx.model_configs import WhisperOnnxConfig\n",
    "# from transformers import AutoConfig\n",
    "\n",
    "# from optimum.exporters.onnx.base import ConfigBehavior\n",
    "# from typing import Dict\n",
    "\n",
    "# class CustomWhisperOnnxConfig(WhisperOnnxConfig):\n",
    "\n",
    "#     def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "#         common_outputs = super().outputs\n",
    "\n",
    "#         if self._behavior is ConfigBehavior.ENCODER:\n",
    "#             for i in range(self._config.encoder_layers):\n",
    "#                 common_outputs[f\"encoder_attentions.{i}\"] = {0: \"batch_size\"}\n",
    "#         elif self._behavior is ConfigBehavior.DECODER:\n",
    "#             for i in range(self._config.decoder_layers):\n",
    "#                 common_outputs[f\"decoder_attentions.{i}\"] = {\n",
    "#                     0: \"batch_size\",\n",
    "#                     2: \"decoder_sequence_length\",\n",
    "#                     3: \"past_decoder_sequence_length + 1\"\n",
    "#                 }\n",
    "#             for i in range(self._config.decoder_layers):\n",
    "#                 common_outputs[f\"cross_attentions.{i}\"] = {\n",
    "#                     0: \"batch_size\",\n",
    "#                     2: \"decoder_sequence_length\",\n",
    "#                     3: \"encoder_sequence_length_out\"\n",
    "#                 }\n",
    "\n",
    "#         return common_outputs\n",
    "\n",
    "\n",
    "#     def torch_to_onnx_output_map(self):\n",
    "#         if self._behavior is ConfigBehavior.ENCODER:\n",
    "#             # The encoder export uses WhisperEncoder that returns the key \"attentions\"\n",
    "#             return {\"attentions\": \"encoder_attentions\"}\n",
    "#         else:\n",
    "#             return {}\n",
    "\n",
    "# model_id = \"openai/whisper-tiny.en\"\n",
    "# config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "# custom_whisper_onnx_config = CustomWhisperOnnxConfig(\n",
    "#         config=config,\n",
    "#         task=\"automatic-speech-recognition\",\n",
    "# )\n",
    "\n",
    "# encoder_config = custom_whisper_onnx_config.with_behavior(\"encoder\")\n",
    "# decoder_config = custom_whisper_onnx_config.with_behavior(\"decoder\", use_past=False)\n",
    "# decoder_with_past_config = custom_whisper_onnx_config.with_behavior(\"decoder\", use_past=True)\n",
    "\n",
    "# custom_onnx_configs={\n",
    "#     \"encoder_model\": encoder_config,\n",
    "#     \"decoder_model\": decoder_config,\n",
    "#     \"decoder_with_past_model\": decoder_with_past_config,\n",
    "# }\n",
    "\n",
    "# main_export(\n",
    "#     model_id,\n",
    "#     output=\"custom_whisper_onnx\",\n",
    "#     no_post_process=True,\n",
    "#     model_kwargs={\"output_attentions\": True},\n",
    "#     custom_onnx_configs=custom_onnx_configs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "from optimum.exporters.onnx.model_configs import TextSeq2SeqOnnxConfig\n",
    "from transformers import AutoConfig\n",
    "from optimum.utils import NormalizedSeq2SeqConfig\n",
    "\n",
    "from optimum.exporters.onnx.base import ConfigBehavior\n",
    "from typing import Dict\n",
    "\n",
    "class CustomOnnxConfig(TextSeq2SeqOnnxConfig):\n",
    "    NORMALIZED_CONFIG_CLASS = NormalizedSeq2SeqConfig\n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_outputs = super().outputs\n",
    "\n",
    "        if self._behavior is ConfigBehavior.ENCODER:\n",
    "            for i in range(self._config.encoder_layers):\n",
    "                common_outputs[f\"encoder_attentions.{i}\"] = {0: \"batch_size\"}\n",
    "        elif self._behavior is ConfigBehavior.DECODER:\n",
    "            for i in range(self._config.decoder_layers):\n",
    "                common_outputs[f\"decoder_attentions.{i}\"] = {\n",
    "                    0: \"batch_size\",\n",
    "                    2: \"decoder_sequence_length\",\n",
    "                    3: \"past_decoder_sequence_length + 1\"\n",
    "                }\n",
    "            for i in range(self._config.decoder_layers):\n",
    "                common_outputs[f\"cross_attentions.{i}\"] = {\n",
    "                    0: \"batch_size\",\n",
    "                    2: \"decoder_sequence_length\",\n",
    "                    3: \"encoder_sequence_length_out\"\n",
    "                }\n",
    "\n",
    "        return common_outputs\n",
    "\n",
    "    @property\n",
    "    def torch_to_onnx_output_map(self):\n",
    "        if self._behavior is ConfigBehavior.ENCODER:\n",
    "            # The encoder export uses WhisperEncoder that returns the key \"attentions\"\n",
    "            return {\"attentions\": \"encoder_attentions\"}\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "model_id = \"facebook/m2m100_418M\"\n",
    "config = AutoConfig.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_onnx_config = CustomOnnxConfig(\n",
    "        config=config,\n",
    "        task=\"text2text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = custom_onnx_config.with_behavior(\"encoder\")\n",
    "decoder_config = custom_onnx_config.with_behavior(\"decoder\", use_past=False)\n",
    "decoder_with_past_config = custom_onnx_config.with_behavior(\"decoder\", use_past=True)\n",
    "\n",
    "custom_onnx_configs={\n",
    "    \"encoder_model\": encoder_config,\n",
    "    \"decoder_model\": decoder_config,\n",
    "    \"decoder_with_past_model\": decoder_with_past_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Automatic task detection to text2text-generation-with-past (possible synonyms are: seq2seq-lm-with-past).\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:141: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_pos > self.weights.size(0):\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:266: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:273: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:305: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Using framework PyTorch: 2.1.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:66: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "Using framework PyTorch: 2.1.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Validating ONNX model custom_whisper_onnx/encoder_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (encoder_attentions.1, encoder_attentions.4, encoder_attentions.5, last_hidden_state, encoder_attentions.8, encoder_attentions.9, encoder_attentions.2, encoder_attentions.11, encoder_attentions.0, encoder_attentions.7, encoder_attentions.6, encoder_attentions.3, encoder_attentions.10)\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (2, 16, 1024) matches (2, 16, 1024)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.0\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.1\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.2\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.3\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.4\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.5\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.6\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.7\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.8\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.9\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.10\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"encoder_attentions.11\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "Validating ONNX model custom_whisper_onnx/decoder_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (cross_attentions.1, cross_attentions.11, decoder_attentions.7, decoder_attentions.5, cross_attentions.10, decoder_attentions.0, decoder_attentions.4, cross_attentions.2, cross_attentions.0, logits, cross_attentions.3, decoder_attentions.9, decoder_attentions.2, cross_attentions.4, cross_attentions.9, decoder_attentions.10, decoder_attentions.3, cross_attentions.7, cross_attentions.6, decoder_attentions.8, cross_attentions.5, decoder_attentions.1, cross_attentions.8, decoder_attentions.11, decoder_attentions.6)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 16, 128112) matches (2, 16, 128112)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.0\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.1\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.2\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.3\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.4\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.5\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.6\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.7\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.8\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.9\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.10\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.11\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.0\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.1\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.2\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.3\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.4\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.5\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.6\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.7\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.8\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.9\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.10\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.11\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "Validating ONNX model custom_whisper_onnx/decoder_with_past_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (present.0.encoder.value, present.11.decoder.key, cross_attentions.1, present.2.encoder.value, present.0.encoder.key, present.6.encoder.key, present.9.decoder.value, present.2.decoder.value, present.10.encoder.key, present.0.decoder.value, present.4.encoder.value, present.7.decoder.key, present.10.decoder.key, decoder_attentions.5, decoder_attentions.6, decoder_attentions.7, cross_attentions.11, present.5.decoder.value, present.6.encoder.value, present.5.encoder.value, present.9.decoder.key, cross_attentions.10, present.10.encoder.value, present.11.decoder.value, present.7.decoder.value, decoder_attentions.0, decoder_attentions.4, cross_attentions.2, present.9.encoder.value, present.3.decoder.value, present.0.decoder.key, logits, present.3.encoder.value, present.7.encoder.key, cross_attentions.0, cross_attentions.3, decoder_attentions.9, present.3.encoder.key, present.3.decoder.key, present.4.decoder.key, present.1.encoder.value, decoder_attentions.2, present.11.encoder.value, present.9.encoder.key, cross_attentions.4, cross_attentions.9, present.4.encoder.key, present.6.decoder.value, present.11.encoder.key, decoder_attentions.3, decoder_attentions.10, present.6.decoder.key, present.1.encoder.key, present.8.encoder.value, cross_attentions.7, cross_attentions.6, present.8.decoder.key, decoder_attentions.8, cross_attentions.5, decoder_attentions.1, cross_attentions.8, present.8.encoder.key, present.4.decoder.value, present.1.decoder.key, present.5.encoder.key, present.10.decoder.value, present.8.decoder.value, decoder_attentions.11, present.1.decoder.value, present.5.decoder.key, present.2.encoder.key, present.7.encoder.value, present.2.decoder.key)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 16, 128112) matches (2, 16, 128112)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.0\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.1\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.2\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.3\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.4\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.5\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.6\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.7\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.8\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.9\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.10\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"decoder_attentions.11\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.0\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.1\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.2\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.3\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.4\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.5\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.6\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.7\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.8\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.9\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.10\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"cross_attentions.11\":\n",
      "\t\t-[✓] (2, 16, 16, 16) matches (2, 16, 16, 16)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "The ONNX export succeeded and the exported model was saved at: custom_whisper_onnx\n"
     ]
    }
   ],
   "source": [
    "main_export(\n",
    "    model_id,\n",
    "    output=\"custom_whisper_onnx\",\n",
    "    no_post_process=True,\n",
    "    model_kwargs={\"output_attentions\": True},\n",
    "    custom_onnx_configs=custom_onnx_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomOnnxConfig object at 0x7ff4858d4220>\n"
     ]
    }
   ],
   "source": [
    "# from optimum.onnxruntime import ORTModelForCustomTasks\n",
    "# model = ORTModelForCustomTasks.from_pretrained(model_id=\"custom_whisper_onnx\", config=custom_onnx_configs, model_save_dir= \"/home/giorgospap/onnx/test/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from optimum.onnxruntime import ORTModelForCustomTasks\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"optimum/sbert-all-MiniLM-L6-with-pooler\")\n",
    "# model = ORTModelForCustomTasks.from_pretrained(\"optimum/sbert-all-MiniLM-L6-with-pooler\")\n",
    "\n",
    "# inputs = tokenizer(\"I love burritos!\", return_tensors=\"np\")\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# last_hidden_state = outputs.last_hidden_state\n",
    "# pooler_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime import InferenceSession\n",
    "onnx_enc_path = \"encoder_model.onnx\"\n",
    "onnx_dec_path = \"decoder_model.onnx\"\n",
    "onnx.checker.check_model(onnx_enc_path)\n",
    "onnx.checker.check_model(onnx_dec_path)\n",
    "# Start onnxruntime sessions\n",
    "enc_session = InferenceSession(onnx_enc_path)\n",
    "dec_session = InferenceSession(onnx_dec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgospap/onnx/onnx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'content-length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:23\u001b[0m, in \u001b[0;36mget_encoder_decoder_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     decoder_sess, encoder_sess \u001b[39m=\u001b[39m _handle_creation_of_sessions()\n\u001b[1;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:45\u001b[0m, in \u001b[0;36m_handle_creation_of_sessions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     path_t5_encoder_tarball \u001b[39m=\u001b[39m _models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39mt5-encoder.tar.gz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     _download_generation_model(path_t5_encoder_tarball)\n\u001b[1;32m     47\u001b[0m \u001b[39m# Checks if decoder is already expanded\u001b[39;00m\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:95\u001b[0m, in \u001b[0;36m_download_generation_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     94\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://t5-onnx-models.s3.amazonaws.com/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 95\u001b[0m size \u001b[39m=\u001b[39m _get_size(url)\n\u001b[1;32m     97\u001b[0m req \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, allow_redirects\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:90\u001b[0m, in \u001b[0;36m_get_size\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     89\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mhead(url, allow_redirects\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(response\u001b[39m.\u001b[39;49mheaders[\u001b[39m'\u001b[39;49m\u001b[39mContent-Length\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/requests/structures.py:52\u001b[0m, in \u001b[0;36mCaseInsensitiveDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store[key\u001b[39m.\u001b[39;49mlower()][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content-length'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/giorgospap/onnx/test.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnxt5\u001b[39;00m \u001b[39mimport\u001b[39;00m GenerativeT5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnxt5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m get_encoder_decoder_tokenizer\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dec_session, enc_session, tokenizer \u001b[39m=\u001b[39m get_encoder_decoder_tokenizer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m generative_t5 \u001b[39m=\u001b[39m GenerativeT5(enc_session, dec_session, tokenizer, onnx\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:28\u001b[0m, in \u001b[0;36mget_encoder_decoder_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m filelist:\n\u001b[1;32m     27\u001b[0m         os\u001b[39m.\u001b[39mremove(f)\n\u001b[0;32m---> 28\u001b[0m     decoder_sess, encoder_sess \u001b[39m=\u001b[39m _handle_creation_of_sessions()\n\u001b[1;32m     30\u001b[0m \u001b[39m# The tokenizer should be the one you trained in the case of fine-tuning\u001b[39;00m\n\u001b[1;32m     31\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-base\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:45\u001b[0m, in \u001b[0;36m_handle_creation_of_sessions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path_t5_encoder\u001b[39m.\u001b[39mexists():\n\u001b[1;32m     44\u001b[0m     path_t5_encoder_tarball \u001b[39m=\u001b[39m _models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39mt5-encoder.tar.gz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     _download_generation_model(path_t5_encoder_tarball)\n\u001b[1;32m     47\u001b[0m \u001b[39m# Checks if decoder is already expanded\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path_t5_decoder\u001b[39m.\u001b[39mexists():\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:95\u001b[0m, in \u001b[0;36m_download_generation_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_generation_model\u001b[39m(path):\n\u001b[1;32m     94\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://t5-onnx-models.s3.amazonaws.com/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 95\u001b[0m     size \u001b[39m=\u001b[39m _get_size(url)\n\u001b[1;32m     97\u001b[0m     req \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, allow_redirects\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m     \u001b[39m# Downloads from S3, reporting progress in bytes to a tqdm progress bar. Units are in bytes. Setting disable to None\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# causes tqdm to check whether the process is attached to a terminal and disable progress bar output if not.\u001b[39;00m\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxt5/api.py:90\u001b[0m, in \u001b[0;36m_get_size\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_size\u001b[39m(url):\n\u001b[1;32m     89\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mhead(url, allow_redirects\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(response\u001b[39m.\u001b[39;49mheaders[\u001b[39m'\u001b[39;49m\u001b[39mContent-Length\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/requests/structures.py:52\u001b[0m, in \u001b[0;36mCaseInsensitiveDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store[key\u001b[39m.\u001b[39;49mlower()][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content-length'"
     ]
    }
   ],
   "source": [
    "from onnxt5 import GenerativeT5\n",
    "from onnxt5.api import get_encoder_decoder_tokenizer\n",
    "dec_session, enc_session, tokenizer = get_encoder_decoder_tokenizer()\n",
    "generative_t5 = GenerativeT5(enc_session, dec_session, tokenizer, onnx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Required inputs (['input_ids', 'attention_mask']) are missing from input feed (['input_features']).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/giorgospap/onnx/test.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/giorgospap/onnx/test.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m enc_session\u001b[39m.\u001b[39;49mrun(\u001b[39mNone\u001b[39;49;00m, input_feed\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39minput_features\u001b[39;49m\u001b[39m\"\u001b[39;49m:source_text})[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:216\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, output_names, input_feed, run_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    Compute the predictions.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m        sess.run([output_name], {input_name: x})\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_input(\u001b[39mlist\u001b[39;49m(input_feed\u001b[39m.\u001b[39;49mkeys()))\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_names:\n\u001b[1;32m    218\u001b[0m         output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n",
      "File \u001b[0;32m~/onnx/onnx/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:198\u001b[0m, in \u001b[0;36mSession._validate_input\u001b[0;34m(self, feed_input_names)\u001b[0m\n\u001b[1;32m    196\u001b[0m         missing_input_names\u001b[39m.\u001b[39mappend(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m missing_input_names:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRequired inputs (\u001b[39m\u001b[39m{\u001b[39;00mmissing_input_names\u001b[39m}\u001b[39;00m\u001b[39m) are missing from input feed (\u001b[39m\u001b[39m{\u001b[39;00mfeed_input_names\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Required inputs (['input_ids', 'attention_mask']) are missing from input feed (['input_features'])."
     ]
    }
   ],
   "source": [
    "encoder_hidden_states = enc_session.run(None, input_feed={\"input_features\":source_text})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_ids = np.array(processor.get_decoder_prompt_ids(), dtype=np.int64)\n",
    "\n",
    "input_feed = {\"input_ids\":input_ids, \"encoder_hidden_states\":encoder_hidden_states}\n",
    "\n",
    "out = dec_session.run(None, input_feed=input_feed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
